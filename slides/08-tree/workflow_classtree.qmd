---
title: "Machine Learning Workflow: Classification Trees"
subtitle: "Biostat 212A"
author: "Dr. Jin Zhou @ UCLA"
date: today
format:
  html:
    theme: cosmo
    embed-resources: true
    number-sections: true
    toc: true
    toc-depth: 4
    toc-location: left
    code-fold: false
engine: knitr
knitr:
  opts_chunk: 
    fig.align: 'center'
    # fig.width: 6
    # fig.height: 4
    message: FALSE
    cache: false
---

Display system information for reproducibility.

::: {.panel-tabset}

#### Python

```{python}
import IPython
print(IPython.sys_info())
```

:::

## Overview

![](https://www.tidymodels.org/start/resampling/img/resampling.svg)


We illustrate the typical machine learning workflow for regression trees using the `Heart` data set from R `ISLR2` package. 

1. Initial splitting to test and non-test sets.

2. Pre-processing of data: not much is needed for regression trees.

3. Tune the cost complexity pruning hyper-parameter(s) using 10-fold cross-validation (CV) on the non-test data.

4. Choose the best model by CV and refit it on the whole non-test data.

5. Final prediction on the test data.

## Heart data

The goal is to predict the binary outcome `AHD` (`Yes` or `No`) of patients.

::: {.panel-tabset}
#### R
```{r}
sessionInfo()
```

```{r}
library(GGally)
library(gtsummary)
library(ranger)
library(tidyverse)
library(tidymodels)
library(ISLR2)

Heart <- read_csv("../data/Heart.csv") 
Heart <-  Heart %>% 
  # first column is patient ID, which we don't need
  select(-1) %>%
  # RestECG is categorical with value 0, 1, 2
  mutate(RestECG = as.character(RestECG)) %>%
  print(width = Inf)
```

```{r}

Heart %>% tbl_summary()
```


#### Python

```{python}
# Load the pandas library
import pandas as pd
# Load numpy for array manipulation
import numpy as np
# Load seaborn plotting library
import seaborn as sns
import matplotlib.pyplot as plt

# Set font sizes in plots
sns.set(font_scale = 1.2)
# Display all columns
pd.set_option('display.max_columns', None)

Heart = pd.read_csv("../data/Heart.csv")
Heart
```

```{python}
# Numerical summaries
Heart.describe(include = 'all')
```

Graphical summary:
```{python}
#| eval: false
# Graphical summaries
plt.figure()
sns.pairplot(data = Heart);
plt.show()
```

:::


## Initial split into test and non-test sets

We randomly split the data in half of test data and another half of non-test data. Stratify on `AHD`.

::: {.panel-tabset}

#### R
```{r}
# For reproducibility
set.seed(212)

data_split <- initial_split(
  Heart, 
  prop = 0.5,
  strata = AHD
  )
data_split

Heart_other <- training(data_split)
dim(Heart_other)

Heart_test <- testing(data_split)
dim(Heart_test)
```

#### Python

```{python}
from sklearn.model_selection import train_test_split

Heart_other, Heart_test = train_test_split(
  Heart, 
  train_size = 0.75,
  random_state = 425, # seed
  stratify = Heart.AHD
  )
Heart_test.shape
Heart_other.shape
```

Separate $X$ and $y$. We will use 13 features.
```{python}
num_features = ['Age', 'Sex', 'RestBP', 'Chol', 'Fbs', 'RestECG', 'MaxHR', 'ExAng', 'Oldpeak', 'Slope', 'Ca']
cat_features = ['ChestPain', 'Thal']
features = np.concatenate([num_features, cat_features])
# Non-test X and y
X_other = Heart_other[features]
y_other = Heart_other.AHD
# Test X and y
X_test = Heart_test[features]
y_test = Heart_test.AHD
```

:::

## Preprocessing (Python) or recipe (R)
- A data dictionary (roughly) is at <https://keras.io/examples/structured_data/structured_data_classification_with_feature_space/>.

- We have following features: 

    - Numerical features: `Age`, `RestBP`, `Chol`, `Slope` (1, 2 or 3), `MaxHR`, `ExAng`, `Oldpeak`, `Ca` (0, 1, 2 or 3).

    - Categorical features coded as integer: `Sex` (0 or 1), `Fbs` (0 or 1), `RestECG` (0, 1 or 2).

    - Categorical features coded as string: `ChestPain`, `Thal`.

- There are missing values in `Ca` and `Thal`. Since missing proportion is not high, we will use simple mean (for numerical feature `Ca`) and mode (for categorical feature `Thal`) imputation.

::: {.panel-tabset}
#### R
```{r}
tree_recipe <- 
  recipe(
    AHD ~ ., 
    data = Heart_other
  ) %>%
  step_naomit(all_predictors()) %>%
  # # create traditional dummy variables (not necessary for random forest in R)
  step_dummy(all_nominal_predictors()) %>%
  # zero-variance filter
  step_zv(all_numeric_predictors()) %>% 
  # # center and scale numeric data (not necessary for random forest)
  step_normalize(all_numeric_predictors()) %>%
  # estimate the means and standard deviations
  prep(training = Heart_other, retain = TRUE)
tree_recipe
```

#### Python

There are missing values in `Ca` (quantitative) and `Thal` (qualitative) variables. We are going to use simple `mean` imputation for `Ca` and `most_frequent` imputation for `Thal`. This is suboptimal. Better strategy is to use multiple imputation.
```{python}
# How many NaNs
Heart.isna().sum()
```

In principle, decision trees should be able to handle categorical predictors. However scikit-learn and xgboost implementations don't allow categorical predictors and require one-hot encoding.

```{python}
from sklearn.preprocessing import OneHotEncoder
from sklearn.impute import SimpleImputer
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline

# Transformer for categorical variables
categorical_tf = Pipeline(steps = [
  ("cat_impute", SimpleImputer(strategy = 'most_frequent')),
  ("encoder", OneHotEncoder())
])

# Transformer for continuous variables
numeric_tf = Pipeline(steps = [
  ("num_impute", SimpleImputer(strategy = 'mean')),
])

# Column transformer
col_tf = ColumnTransformer(transformers = [
  ('num', numeric_tf, num_features),
  ('cat', categorical_tf, cat_features)
])
```

:::

## Model

::: {.panel-tabset}

#### R

```{r}
classtree_mod <- decision_tree(
  cost_complexity = tune(),
  tree_depth = tune(),
  min_n = 5,
  mode = "classification",
  engine = "rpart"
  ) 

```

#### Python

```{python}
from sklearn.tree import DecisionTreeClassifier, plot_tree

classtree_mod = DecisionTreeClassifier(
  criterion = 'gini',
  random_state = 425
  )
```

:::

## Pipeline (Python) or workflow (R)

Here we bundle the preprocessing step (Python) or recipe (R) and model.

::: {.panel-tabset}


#### R

```{r}
tree_wf <- workflow() %>%
  add_recipe(tree_recipe) %>%
  add_model(classtree_mod) 
tree_wf
```

#### Python

```{python}
from sklearn.pipeline import Pipeline

pipe = Pipeline(steps = [
  ("col_tf", col_tf),
  ("model", classtree_mod)
  ])
pipe
```


:::

## Tuning grid

`ccp_alpha` is the Minimal Cost-Complexity Pruning parameter. Greater values of `ccp_alpha` increase the number of nodes pruned.

::: {.panel-tabset}

#### R
```{r}
tree_grid <- grid_regular(cost_complexity(),
                          tree_depth(),
                          levels = c(100,5))
```




#### Python

```{python}
# Tune hyper-parameter(s)
ccp_alpha_grid = np.linspace(start = 0.0, stop = 0.05, num = 100)
tuned_parameters = {
  "model__ccp_alpha": ccp_alpha_grid
  }
tuned_parameters  
```

:::

## Cross-validation (CV)

::: {.panel-tabset}

#### R
Set cross-validation partitions.
```{r}
set.seed(212)

folds <- vfold_cv(Heart_other, v = 5)
folds
```


Fit cross-validation.
```{r}
tree_fit <- tree_wf %>%
  tune_grid(
    resamples = folds,
    grid = tree_grid,
    metrics = metric_set(accuracy, roc_auc)
    )
tree_fit
```

Visualize CV results:
```{r}
tree_fit %>%
  collect_metrics() %>%
  print(width = Inf) %>%
  filter(.metric == "roc_auc") %>%
  mutate(tree_depth = as.factor(tree_depth)) %>%
  ggplot(mapping = aes(x = cost_complexity, y = mean, color = tree_depth)) +
  geom_point() + 
  geom_line() + 
  labs(x = "cost_complexity", y = "CV ROC AUC", color = "tree_depth") 
```
#### Python

Set up CV partitions and CV criterion.

```{python}
from sklearn.model_selection import GridSearchCV

# Set up CV
n_folds = 5
search = GridSearchCV(
  pipe,
  tuned_parameters,
  cv = n_folds, 
  scoring = "roc_auc",
  # Refit the best model on the whole data set
  refit = True
  )
```

Fit CV. This is typically the most time-consuming step.
```{python}
# Fit CV
search.fit(X_other, y_other)
```

Visualize CV results.
```{python}
#| eval: true
#| code-fold: true
cv_res = pd.DataFrame({
  "ccp_alpha": np.array(search.cv_results_["param_model__ccp_alpha"]),
  "auc": search.cv_results_["mean_test_score"]
  })

plt.figure()
sns.relplot(
  # kind = "line",
  data = cv_res,
  x = "ccp_alpha",
  y = "auc"
  ).set(
    xlabel = "CCP Alpha",
    ylabel = "CV AUC"
);
plt.show()
```

Best CV AUC:
```{python}
search.best_score_
```

The training accuracy is
```{python}
from sklearn.metrics import accuracy_score, roc_auc_score

accuracy_score(
  y_other,
  search.best_estimator_.predict(X_other)
  )
```

:::

## Finalize our model

Now we are done tuning. Finally, letâ€™s fit this final model to the whole training data and use our test data to estimate the model performance we expect to see with new data.

::: {.panel-tabset}
#### R
```{r}
tree_fit %>%
  show_best("roc_auc")
```
Let's select the best model.
```{r}
best_tree <- tree_fit %>%
  select_best("roc_auc")
best_tree
```

```{r}
# Final workflow
final_wf <- tree_wf %>%
  finalize_workflow(best_tree)
final_wf
```

```{r}
# Fit the whole training set, then predict the test cases
final_fit <- 
  final_wf %>%
  last_fit(data_split)
final_fit
```

```{r}
# Test metrics
final_fit %>% 
  collect_metrics()
```

#### Python

Since we called `GridSearchCV` with `refit = True`, the best model fit on the whole non-test data is readily available.
```{python}
search.best_estimator_
```

Visualize the best classification tree.
```{python}
#| eval: true

features = np.concatenate([
    features[:-2], 
    ['ChestPain:asymptomatic', 'ChestPain:nonanginal', 'ChestPain:nontypical', 'ChestPain:typical'],
    ['Thal:fixed', 'Thal:normal', 'Thal:reversable']
    ])

plt.figure()
plot_tree(
  search.best_estimator_['model'],
  feature_names = features
  );
plt.show()
```
Feature importances:
```{python}
vi_df = pd.DataFrame({
  "feature": features,
  "vi": search.best_estimator_['model'].feature_importances_
  })

plt.figure()
sns.barplot(
  data = vi_df,
  x = "feature",
  y = "vi"
  ).set(
    xlabel = "Feature",
    ylabel = "VI"
);
plt.xticks(rotation = 90);
plt.show()
```

The final AUC on the test set is
```{python}
roc_auc_score(
  y_test,
  search.best_estimator_.predict_proba(X_test)[:, 1]
  )
```

The final classification accuracy on the test set is
```{python}
accuracy_score(
  y_test, 
  search.best_estimator_.predict(X_test)
  )
```

:::

## Visualize the final model
```{r}
library(rpart.plot)
final_tree <- extract_workflow(final_fit)
final_tree

final_tree %>%
  extract_fit_engine() %>%
  rpart.plot(roundint = FALSE)
```

```{r}
library(vip)

final_tree %>% 
  extract_fit_parsnip() %>% 
  vip()
```